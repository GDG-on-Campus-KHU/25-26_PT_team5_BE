# 1. ìš”ì¦˜ IT
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import kss


MODEL_NAME = "skt/kobert-base-v1"
device = torch.device("cpu")


# 1) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
print("â–¶ KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.to(device)
model.eval()
print("â–¶ ë¡œë“œ ì™„ë£Œ!")

# 2) í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
def split_sentences_kor(text: str):
    """
    ê¸´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬.
    kssë¥¼ ì‚¬ìš©í•´ì„œ ì•ˆì „í•˜ê²Œ ë¬¸ì¥ ë¶„ë¦¬.
    """
    sentences = kss.split_sentences(text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


# 3) ë¬¸ì¥ë“¤ì„ KoBERT CLS ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
def encode_sentences(sentences, batch_size: int = 8, max_length: int = 256):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ -> CLS ì„ë² ë”© (numpy array: [num_sent, hidden_dim])
    """
    all_embeddings = []

    with torch.no_grad():
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i+batch_size]

            enc = tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )

            # ğŸ” ë””ë²„ê·¸: input_ids ë²”ìœ„ í™•ì¸
            ids = enc["input_ids"]
            #print("min id:", ids.min().item(), "max id:", ids.max().item())

            # ğŸ”¥ ì¤‘ìš”: token_type_ids ê°•ì œë¡œ ì œê±°
            if "token_type_ids" in enc:
                #print("-> token_type_ids ì œê±°")
                enc.pop("token_type_ids")

            enc = {k: v.to(device) for k, v in enc.items()}

            outputs = model(**enc)
            # BERTì˜ [CLS] í† í° ë²¡í„° ì‚¬ìš©
            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch, hidden]
            all_embeddings.append(cls_embeddings.cpu().numpy())

    return np.vstack(all_embeddings)  # [num_sent, hidden_dim]


# 4) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì¤‘ìš”í•œ ë¬¸ì¥ top_k ì¶”ì¶œ
def summarize_kobert(text: str, top_k: int = 5):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼
    1) ë¬¸ì¥ ë¶„ë¦¬
    2) ê° ë¬¸ì¥ì„ KoBERTë¡œ ì„ë² ë”©
    3) ë¬¸ì¥ ì„ë² ë”© vs ë¬¸ì„œ í‰ê·  ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    4) ìœ ì‚¬ë„ê°€ í° ìƒìœ„ top_k ë¬¸ì¥ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ë½‘ê¸°
    """
    sentences = split_sentences_kor(text)

    if len(sentences) == 0:
        return "", []

    # ë¬¸ì¥ ìˆ˜ê°€ top_kë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëƒ¥ ì „ì²´ ë°˜í™˜
    if len(sentences) <= top_k:
        summary = " ".join(sentences)
        return summary, sentences

    print(f"â–¶ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}ê°œ")
    sent_embs = encode_sentences(sentences)  # [N, D]

    # ë¬¸ì„œ ì„ë² ë”© = ë¬¸ì¥ ì„ë² ë”© í‰ê· 
    doc_emb = sent_embs.mean(axis=0, keepdims=True)  # [1, D]

    # L2 ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    def l2norm(x, axis):
        return x / (np.linalg.norm(x, axis=axis, keepdims=True) + 1e-8)

    sent_norm = l2norm(sent_embs, axis=1)  # [N, D]
    doc_norm = l2norm(doc_emb, axis=1)     # [1, D]

    sims = (sent_norm @ doc_norm.T).squeeze(1)  # [N]

    # ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ top_k ë¬¸ì¥ ì¸ë±ìŠ¤
    top_idx = sims.argsort()[::-1][:top_k]
    # ë¬¸ì„œ ì›ë˜ ìˆœì„œ ìœ ì§€ (ìš”ì•½ë¬¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì½íˆë„ë¡)
    top_idx_sorted = sorted(top_idx)

    summary_sentences = [sentences[i] for i in top_idx_sorted]
    summary_text = " ".join(summary_sentences)

    return summary_text, summary_sentences

def crawl_news():
    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=options)

    provider = "yozm"
    url = "https://yozm.wishket.com/magazine/list/new/"
    driver.get(url)
    wait = WebDriverWait(driver, 10)

    # í˜ì´ì§€ ë¡œë”© ê¸°ë‹¤ë¦¬ê¸° (ì ë‹¹í•œ ìƒìœ„ ì—˜ë¦¬ë¨¼íŠ¸ ê¸°ì¤€ìœ¼ë¡œ)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "article")))

    articles = driver.find_elements(By.CSS_SELECTOR, "article")

    results = []

    for art in articles:
        try:
            # ì´ article ì•ˆì— '1ì¼ ì „'ì´ ìˆëŠ”ì§€ í™•ì¸
            date_el = art.find_element(
                By.XPATH,
                ".//span[contains(normalize-space(.), '1ì¼ ì „')]"
            )
        except:
            # ì´ ì¹´ë“œì—ëŠ” '1ì¼ ì „'ì´ ì—†ìŒ â†’ ìŠ¤í‚µ
            continue

        # ì œëª©
        title_el = art.find_element(By.TAG_NAME, "h3")
        title = title_el.text.strip()

        # ë‚ ì§œ (ì—¬ê¸°ì„œëŠ” '1ì¼ ì „'ì¼ ê²ƒ)
        date_text = date_el.text.strip()

        # ë§í¬ (ì¹´ë“œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” a íƒœê·¸)
        link_el = art.find_element(
            By.XPATH,
            ".//a[@data-testid='contentsItem-item-link']"
        )
        href = link_el.get_attribute("href")

        thumbnail_url = None
        try:
            # column ìŠ¤íƒ€ì¼ ì¹´ë“œì— í•´ë‹¹
            img_el = art.find_element(
                By.XPATH,
                ".//div[@data-testid='article-column-item--image']//img"
            )
            thumbnail_url = img_el.get_attribute("src")
        except Exception:
            # ìœ„ êµ¬ì¡°ê°€ ì—†ë‹¤ë©´, ì¹´ë“œ ì•ˆì˜ object-cover ì´ë¯¸ì§€ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©
            try:
                img_el = art.find_element(
                    By.XPATH,
                    ".//img[contains(@class, 'object-cover')]"
                )
                thumbnail_url = img_el.get_attribute("src")
            except Exception:
                thumbnail_url = None  # ì •ë§ ì—†ìœ¼ë©´ None

        results.append({
            "title": title,
            "date": date_text,
            "link": href,
            "provider" : provider,
            'thumbnail_url' : thumbnail_url

        })

    print(results)
    detail_results = []

    for item in results:   # list_resultsëŠ” ì•ì—ì„œ ëª¨ì•„ë‘” {link, thumbnail, ...}
        driver.get(item["link"])
        def parse_article_detail(driver, timeout=15):
            data = {}
            local_wait = WebDriverWait(driver, timeout)

            # 1) ì œëª©
            # <h1 class="... typo-title3 desktop:typo-title2">...</h1>
            title_el = driver.find_element(
                By.CSS_SELECTOR,
                "h1.typo-title3, h1.typo-title2"
            )
            data["title"] = title_el.text.strip()

            # 2) ê¸€ì“´ì´ (ì‘ì„±ì)
            # <span ... data-testid="contents-author-name">FEConf</span>
            author_el = driver.find_element(
                By.CSS_SELECTOR,
                "span[data-testid='contents-author-name']"
            )
            data["author"] = author_el.text.strip()

            # 3) ê²Œì‹œ ë‚ ì§œ (ìƒëŒ€ ì‹œê°„: '1ì¼ ì „')
            # <span class="... typo-body2 ...">1ì¼ ì „</span>
            date_el = driver.find_element(
                By.XPATH,
                "//span[contains(@class, 'typo-body2') and contains(normalize-space(.), 'ì¼ ì „')]"
            )
            data["posted_at"] = date_el.text.strip()   # ì˜ˆ: '1ì¼ ì „'

            # 4) ì¹´í…Œê³ ë¦¬
            # <a data-testid="category-link" ...><span ...>ê°œë°œ</span></a>
            category_el = driver.find_element(
                By.CSS_SELECTOR,
                "a[data-testid='category-link'] span"
            )
            data["category"] = category_el.text.strip()   # ì˜ˆ: 'ê°œë°œ'

            # 5) ë³¸ë¬¸ ë‚´ìš©
            # <section id="article-detail-wrapper"> ... ì—¬ê¸° ì•ˆì˜ p, h3, h4, blockquote ë“± ì „ì²´ í…ìŠ¤íŠ¸
            content_section = wait.until(EC.presence_of_element_located((
                By.CSS_SELECTOR,
                "section#article-detail-wrapper"
            )))

            # ğŸ‘‰ ë¬¸ë‹¨ ê°œìˆ˜ ë„ˆë¬´ ë¹¡ë¹¡í•˜ê²Œ ë³´ì§€ ë§ê³ , ì‹¤íŒ¨í•´ë„ ê·¸ëƒ¥ ì§„í–‰
            try:
                local_wait.until(
                    lambda d: len(
                        d.find_elements(
                            By.CSS_SELECTOR,
                            "section#article-detail-wrapper p.typo-contents2"
                        )
                    ) >= 1     # ìµœì†Œ 1ê°œë§Œ ë‚˜ì˜¤ë©´ í†µê³¼
                )
            except TimeoutException:
                # ë¬¸ë‹¨ì´ ì ê±°ë‚˜ ëŠ¦ê²Œ ë– ë„ ê·¸ëƒ¥ í˜„ì¬ ìˆëŠ” ê²ƒë§Œ ê¸ê³  ë„˜ì–´ê°€ê¸°
                pass

            paragraph_els = content_section.find_elements(
            By.XPATH,
            ".//p | .//h3 | .//h4 | .//blockquote"
            )

            paragraphs = [el.text.strip() for el in paragraph_els if el.text.strip()]

            # ì„¹ì…˜ ì•ˆì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ í¬í•¨í•´ì„œ ê°€ì ¸ì˜¤ê¸°
            full_text = content_section.text.strip()
            data["content_raw"] = full_text

            data["content_paragraphs"] = paragraphs

            data["content_raw"] = "\n\n".join(paragraphs)

            return data
        try:
            article_data = parse_article_detail(driver, timeout=15)
        except TimeoutException:
            print("[WARN] ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨, ìŠ¤í‚µ:", item["link"])
            continue
        except Exception as e:
            print("[ERROR] ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬, ìŠ¤í‚µ:", item["link"], e)
            continue

        detail_results.append({
            "url": item["link"],
            "title": article_data["title"],
            "author": article_data["author"],
            "posted_at": article_data["posted_at"],
            "category": article_data["category"],
            "content": article_data["content_raw"],  # ë‚˜ì¤‘ì— ìš”ì•½ ëª¨ë¸ì— ë„£ì„ ì›ë¬¸
            "thumbnail_url": item["thumbnail_url"],
        })
    driver.quit()

    # KoBERT ìš”ì•½
    for article in detail_results:
        text = article.get("content", "")
        if not text:
            article["summary"] = ""
            continue
        summary, _ = summarize_kobert(text, top_k=7)
        article["summary"] = summary

    return detail_results


if __name__ == "__main__":
    data = crawl_news()
    df = pd.DataFrame(data)
    df.to_json(
        "news_output.json",
        orient="records",
        force_ascii=False,
        indent=2,
    )
    print("news_output.json ì €ì¥ ì™„ë£Œ!")

    # 3) CSV / JSON ì €ì¥
    df.to_json(
        "news_output.json",
        orient="records",
        force_ascii=False,
        indent=2,
    )

    print("news_output.json ì €ì¥ ì™„ë£Œ!")